---
title: '2.Gene expression anaylsis: Creating random forest models on lab data'
author: "Fay Webster"
date: '2022-05-27'
geometry: margin=2cm
output:
  pdf_document: default
  theme: journal
  toc: yes
  toc_float: yes
  html_document: null
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,  warning = FALSE)
```


# Aim: 
- Predicting health impact of infections utilizing immune parameters as predictors 
- Predicted variable: WL as a proxy of health 
- To do that we are using immune data from experimental lab infections. 
- We are training random forest models on the immune data from experimental lab infections 
- And we test them on the field.
- We then compare the differences in the predicted health impact among non-hybrid and hybrid mice. 


In this document I am preparing the models using the lab data only. 

#  Load necessary libraries:

```{r libraries, message = FALSE, warnings = FALSE}
#install.packages("optimx", version = "2021-10.12") # this package is required for 
#the parasite load package to work
library(tidyverse)
library(tidyr)
library(dplyr)
library(cowplot)
library(randomForest)
library(ggplot2)
library(caret)
library(ggpubr)
library(rfUtilities) # Implements a permutation test cross-validation for 
# Random Forests models
```


# Laboratory data 

## Importing the data

We start with the data from experimental lab infections. 

```{r import_data,  message = FALSE, warnings = FALSE }
# Here we import the cleaned data set from the previous script derived from the 
# data set challenge infections 
g <- 
  read.csv("https://raw.githubusercontent.com/fayweb/Eimeria_mouse_immunity/main/output_data/gene_expression/data_products/clean_gene_expression.csv")


# vectors for selecting gene columns
Genes <- c("IFNy", "CXCR3_bio", "IL.6", "IL.10", "IL.13", "IL.10", "IL.13", 
           "IL1RN", "CASP1", "CXCL9", "IDO1", "IRGM1", "MPO", "MUC2", "MUC5AC", 
           "MYD88", "NCR1", "PRF1", "RETNLB", "SOCS1", "TICAM1", "TNF")

```



## Data cleaning / preparation

```{r cleaning}
# we need to change the  in challenge infections to a factor
g$Parasite_challenge <- as.factor(g$Parasite_challenge)
g$Eim_MC <- as.factor(g$Eim_MC)

# Here I create a new column, where we get the actual infection status
# According to the melting curve for eimeria 
g <- g %>%
  dplyr::mutate(current_infection = case_when(
    Parasite_challenge == "E_ferrisi" & Eim_MC == "TRUE" ~ "E_ferrisi",
    Parasite_challenge == "E_ferrisi" & Eim_MC == "FALSE" ~ "uninfected",
    Parasite_challenge == "E_falciformis" & Eim_MC == "TRUE" ~ "E_falciformis",
    Parasite_challenge == "E_falciformis" & Eim_MC == "FALSE" ~ "uninfected",
    Parasite_challenge == "uninfected" & Eim_MC == "TRUE" ~ "infected_eimeria",
    Parasite_challenge == "uninfected" & Eim_MC == "FALSE" ~ "uninfected",
    TRUE ~ ""
  ))


# create variable maximum weight loss instead of maximum relative weight loss
g <- g %>% dplyr::mutate(max_WL = 100 - max_WL)


```

# Imputation of missing data 


## Imputing missing data + cleaning

Here, I am using a function from the random forest package, 
rfImpute which utilizes random forests to impute missing data in the other 
variables. 

The variables used for imputing mainly the immune gene expression are the 
current infection, the state of Eimeria infection, oocysts and the non-missing 
genes.


```{r imputing}
#Start by selecting only the genes and the maximum weight loss for each mouse
# Apparently the relative end weight doesn't work so well for predictions

g.1 <- g %>% dplyr::select(c(all_of(Genes), max_WL,
                             mouse_strain, CD4, Treg, Div_Treg, 
                             Treg17, Th1, Div_Th1, Th17, Div_Th17, CD8, Act_CD8, 
                             Div_Act_CD8, IFNy_CD4, IFNy_CD8,Treg_prop, 
                             IL17A_CD4))

sapply(g.1, function(x) sum(is.na(x)))

g.1  <- g.1 %>% mutate_if(is.character, as.factor)
g.1  <- g.1 %>% mutate_if(is.integer, as.numeric)


# to get reproducible results we use a seed
set.seed(42)

# We want the maximum weight loss to be predicted by the data ina ll of 
# the other columns

# iter = how many random forests are needed, in theory 6 are enough
g.imputed <- rfImpute(max_WL ~ ., data = g.1, iter = 6)



g_minus <- g %>% 
  dplyr::select(-c(all_of(Genes), max_WL,
                             mouse_strain, CD4, Treg, Div_Treg, 
                             Treg17, Th1, Div_Th1, Th17, Div_Th17, CD8, Act_CD8, 
                             Div_Act_CD8, IFNy_CD4, IFNy_CD8,Treg_prop, 
                             IL17A_CD4))

#full data set containing the imputed gene expression data
g.imputed <- cbind(g_minus, g.imputed)

```


How many mice are in the infection planning?
```{r infection_plan}
g.imputed  %>%
  group_by(Parasite_challenge) %>%
  summarize(length(EH_ID))
  
``` 
How many mice are indeed infected?

```{r infected}
g.imputed %>% 
  filter(infection == "challenge") %>%
  group_by(current_infection) %>%
  summarize(length(EH_ID))
  
```
I guess mice got mixed up here?

## Splitting data into training and testing sets 
Splitting between training and testing:
- Assess model performance on unseen data
- Avoid over-fitting 


# Random forest for predicting percentage of maximum weight loss

## Dividing data into training and testing 

```{r Diving_testing_training}

Genes <- c("IFNy",  "IL.6", "IL.10", "IL.13", "IL.10", "IL.13", "IL1RN", 
           "CASP1", "CXCL9", "IDO1", "IRGM1", "MPO", "MUC2", "MUC5AC", "MYD88", 
           "NCR1", "PRF1", "RETNLB", "SOCS1", "TICAM1", "TNF")

g.imputed_full <- g.imputed

write.csv(g.imputed_full, 
          "output_data/gene_expression/data_products/lab_imputed_gene_expression.csv", 
          row.names = FALSE)

#select the relevant columns:
g.imputed <- g.imputed %>%
  dplyr::select(c(max_WL, all_of(Genes))) 

# split data into training and test

set.seed(123) # this will help us reproduce this random assignment

# in this way we can pick the random numbers

training.samples <- g.imputed$max_WL%>%
  createDataPartition(p = .7, 
                      list = FALSE) 
# this is the partiicition! In this case 0.7 = training data and 0.3 = testing
# we don't want to get a list in return



train.data <- g.imputed[training.samples, ] 
test.data <- g.imputed[-training.samples, ] 


```



## Building the model

```{r  predicting_weight_loss_model}
#train the model
weight_loss_predict <- randomForest(max_WL ~., data = train.data, 
                                    proximity = TRUE, ntree = 1000) 
# ntree = number of trees     

# save the model 
save(weight_loss_predict, file =  "r_scripts/models/predict_weight_loss.RData")

print(weight_loss_predict)
```

Plotting the weight_loss_predict will illustrate the error rate as we average 
across more trees and shows that our error rate stabalizes with around 200 trees.


## Model - quality testing

### Cross-validation 

MSE: As a brief explanation, mean squared error (MSE) is the average of the 
summation of the squared difference between the actual output value and the 
predicted output value. Our goal is to reduce the MSE as much as possible. 

Variance explained: %explained variance is a measure of how well out-of-bag 
predictions explain the target variance of the training set.

```{r}
predict_WL_cv <- rf.crossValidation(x = weight_loss_predict, xdata = train.data, 
                                    p = 0.10, n = 99, ntree = 501)

predict_WL_cv$fit.var.exp


par(mfrow=c(2,2))

plot(predict_WL_cv) 

# Root Mean Squared Error (observed vs. predicted) from each Bootstrap 
# iteration (cross-validation)
plot(predict_WL_cv, stat = "mse")

#Percent variance explained from specified fit model
plot(predict_WL_cv, stat = "var.exp")

#Mean Absolute Error from each Bootstrapped model
plot(predict_WL_cv, stat = "mae")

```

```{r}
plot(weight_loss_predict)
```
The plotted error rate above is based on the OOB sample error 
and can be accessed directly at m1$mse. Thus, we can find which number of trees 
providing the lowest error rate, which is 257 trees providing an weight 
error of 5.024738.

```{r}
# number of trees with lowest MSE
which.min(weight_loss_predict$mse)
## [1] 257

# RMSE of this optimal random forest
sqrt(weight_loss_predict$mse[which.min(weight_loss_predict$mse)])
## [1] 5.024738
```

### https://uc-r.github.io/s
RandomForest also allows us to use a validation set to measure predictive
accuracy if we did not want to use the OOB samples. 



Tutorial: 
https://hackernoon.com/random-forest-regression-in-r-code-and-interpretation

Random forest regression in R provides two outputs: decrease in mean square 
error (MSE) and node purity. Prediction error described as MSE is based on 
permuting out-of-bag sections of the data per individual tree and predictor, 
and the errors are then averaged. In the regression context, Node purity is the 
total decrease in residual sum of squares when splitting on a variable averaged 
over all trees (i.e. how well a predictor decreases variance). MSE is a more 
reliable measure of variable importance. If the two importance metrics show 
different results, listen to MSE. If all of your predictors are numerical, then 
it shouldnâ€™t be too much of an issue


Mean Decrease Gini (IncNodePurity) - This is a measure of variable importance 
based on the Gini impurity index used for the calculating the splits in trees.

Improving Your Model
Your model depends on the quality of your dataset and the type of Machine 
Learning algorithm used. Therefore, to improve the accuracy of your model, 
you should:

Check what attributes affect our model the most and what variables to leave out 
in future analysis
Find out what other attributes affect a person's wage; we can use as predictors 
in future analysis
Tweak the algorithm (e.g. change the ntree value)
Use a different machine learning algorithm
If any of these reduces the RMSE significantly, you have succeeded in improving 
your model!

```{r include = FALSE, message = FALSE, echo = FALSE}
### Visualize variable importance ----------------------------------------------

#Call importance() function on the model model to check how the attributes used 
# as predictors affect our weight_loss_predict
importance(weight_loss_predict)

weight_loss_predict$mse
## S3 method for class 'randomForest'
plot(weight_loss_predict, type = "l", main=deparse(substitute(x)))

varImpPlot(weight_loss_predict)

# Get variable importance from the weight_loss_predict fit
ImpData <- as.data.frame(importance(weight_loss_predict))
ImpData$Var.Names <- row.names(ImpData)

```



# Application of weight_loss_predict
## Using the testing data

Let's now make some predictions using our test data.


```{r }
#The predict() function in R is used to predict the values based on the 
# input data.
predictions <- predict(weight_loss_predict, test.data)

# assign test.data to a new object, so that we can make changes
result <- test.data

#add the new variable of predictions to the result object
result <- cbind(result, predictions)

#add the results to a data frame containing test data and the prediction
result <- cbind(g[row.names(result), ], predictions)


# what is the correlation between predicted and actual data?
cor(result$max_WL, result$predictions, 
    method = c("pearson", "kendall", "spearman"))
```



## Visualizing the predictions



```{r predictions_weight_loss_predict_wL }
# trying to find a way to represent the delta ct for the negative ones
# please find a better way to do this
result <- result %>% 
    dplyr::mutate(Infection_intensity = case_when(
       Parasite_challenge == "uninfected" ~ -9,
       TRUE ~ delta
    ))

cor(result$predictions, result$max_WL)

result   %>%
  ggplot() +
<<<<<<< HEAD
  geom_point(aes(x = predictions, y = max_WL, color = current_infection, size = delta2)) +
  labs(x = "Predictions: Maximum weight loss", y = "Observed: Maximum weight loss") +
=======
  geom_point(aes(x = predictions, y = max_WL, 
                 color = Parasite_challenge, size = Infection_intensity)) +
  labs(x = "Predictions: Maximum weight loss", 
       y = "Observed: Maximum weight loss") +
>>>>>>> 9bce7f3bffa8a033f8b31bab042a0322618de2fc
    theme_bw()

cor.test(result$predictions, result$max_WL, method = "spearman")
  
```



# Using the same method to predict either Melting curve or infecting parasite 
# (2nd validation)

As a second part I am using the same method to predict either infection with
Eimeria in general or the species of eimeria. 



##  Predicting eimeria species



### Predicing parasite: splliting into training and testing

```{r }

g.imputed_full$Parasite_challenge <- 
  as.factor(g.imputed_full$Parasite_challenge)

#select the relevant columns:
g.imputed_parasite <- g.imputed_full %>%
  dplyr::select(c(Parasite_challenge, all_of(Genes)))

g.imputed_parasite <- g.imputed_parasite %>%
    dplyr::select(-CXCR3_bio)
# split data into training and test
set.seed(123) # this will help us reproduce this random assignment
# in this way we can pick the random numbers
training.samples_parasite <- g.imputed_parasite$Parasite_challenge%>%
  createDataPartition(p = .7, list = FALSE) 
train.data_parasite <- g.imputed_parasite[training.samples, ] 
test.data_parasite <- g.imputed_parasite[-training.samples, ] 


```

### Building the model_Parasite

```{r }
#train the model
model_Parasite <- randomForest(Parasite_challenge ~., 
                               data = train.data_parasite, proximity = TRUE,
                      ntree = 1500) # number of trees

# save the model 
save(model_Parasite, file =  "r_scripts/models/predict_infecting_parasite.RData")

print(model_Parasite)
```
OOB = 46.43, this means that only 53 % of our predictions are accurate


### Quality checks


#### Cross-validation 

MSE: As a brief explanation, mean squared error (MSE) is the average of the 
summation of the squared difference between the actual output value and the 
predicted output value. Our goal is to reduce the MSE as much as possible. 

Variance explained: %explained variance is a measure of how well out-of-bag 
predictions explain the target variance of the training set.

```{r}
model_Parasite_cv <- rf.crossValidation(x = model_Parasite, xdata =  
                                          train.data_parasite, 
                                    p = 0.10, n = 99, ntree = 501)

model_Parasite_cv$fit.var.exp


# Plot cross validation versus model producers accuracy
par(mfrow=c(1,2)) 
  plot(model_Parasite_cv, type = "cv", main = "CV producers accuracy")
  plot(model_Parasite_cv, type = "model", main = "Model producers accuracy")

  # Plot cross validation versus model oob
#par(mfrow=c(1,2)) 
 # plot(model_Parasite_cv, type = "cv", stat = "oob", main = "CV oob error")
  #plot(model_Parasite_cv, type = "model", stat = "oob", 
   #    main = "Model oob error")	  

```

```{r}
plot(model_Parasite)
```

### Testing the model: Predictions


```{r }
#The predict() function in R is used to predict the values based on the input 
# data.
predictions_parasite <- predict(model_Parasite, test.data_parasite)
# assign test.data to a new object, so that we can make changes
result_parasite <- test.data_parasite
#add the new variable of predictions to the result object
result_parasite <- cbind(result_parasite, predictions_parasite)
#add the results to a data frame containing test data and the prediction
result_parasite <- cbind(g[row.names(result_parasite), ], predictions_parasite)
```


### Visualizing predictions_parasite



```{r }

conf_matrix_parasite <- 
  confusionMatrix(
    result_parasite$predictions_parasite,
    reference = result_parasite$Parasite_challenge)

print(conf_matrix_parasite)

conf_matrix_parasite$table

plt <- as.data.frame(conf_matrix_parasite$table)
plt$Prediction <- factor(plt$Prediction, levels=rev(levels(plt$Prediction)))

ggplot(plt, aes(x = Prediction, y =  Reference, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="darkturquoise") +
        labs(x = "Predictions",y = "Reference") 

```


## Predicting for Melting curve



### Split the data again into training and testing

```{r splitting_data}
#select the relevant columns:
g.imputed_mc <- g.imputed_full %>%
  dplyr::select(c(Eim_MC, all_of(Genes)))


# split data into training and test
set.seed(123) # this will help us reproduce this random assignment
# in this way we can pick the random numbers
training.samples_mc <- g.imputed_mc$Eim_MC %>%
  createDataPartition(p = .7, list = FALSE)
train.data_mc <- g.imputed_mc[training.samples, ] 
test.data_mc <- g.imputed_mc[-training.samples, ] 
```

### Building the model

```{r }
#train the model
model_mc <- randomForest(Eim_MC ~., data = train.data_mc, proximity = TRUE,
                      ntree = 1500) # number of trees

# save the model 
save(model_mc, file =  "r_scripts/models/predicting_mc.RData")

                      
print(model_mc)
```


#### Cross-validation 

MSE: As a brief explanation, mean squared error (MSE) is the average of the 
summation of the squared difference between the actual output value 
and the predicted output value. Our goal is to reduce the MSE as much as possible. 

Variance explained: %explained variance is a measure of how well out-of-bag 
predictions explain the target variance of the training set.

```{r}
model_mc_cv <- rf.crossValidation(x = model_mc, xdata =  train.data_mc, 
                                    p = 0.10, n = 99, ntree = 501)

model_mc_cv$fit.var.exp


# Plot cross validation versus model producers accuracy
par(mfrow=c(1,2)) 
  plot(model_mc_cv, type = "cv", main = "CV producers accuracy")
  plot(model_mc_cv, type = "model", main = "Model producers accuracy")

  # Plot cross validation versus model oob
par(mfrow=c(1,2)) 
  plot(model_mc_cv, type = "cv", stat = "oob", main = "CV oob error")
  plot(model_mc_cv, type = "model", stat = "oob", main = "Model oob error")	  

```



```{r}
plot(model_mc)
```

## Test the model


### Making predictions


```{r }
#The predict() function in R is used to predict the values based on the input data.
predictions_mc <- predict(model_mc, test.data_mc)
# assign test.data to a new object, so that we can make changes
result_mc <- test.data_mc
#add the new variable of predictions to the result object
result_mc <- cbind(result_mc, predictions_mc)
#add the results to a data frame containing test data and the prediction
result_mc <- cbind(g[row.names(result_mc), ], predictions_mc)

```



### Visualizations



```{r }

conf_matrix_mc <- 
  confusionMatrix(result_mc$predictions_mc, reference = result_mc$Eim_M)

print(conf_matrix_mc)

conf_matrix_mc$table

<<<<<<< HEAD
```{r}
result_field %>% ggplot(aes(x = predictions_field)) +
  geom_histogram(binwidth = 1.5)



```

```{r}
result_field %>%
    ggplot(aes(x = HI , y = predictions_field , color = Sex)) +
    geom_smooth() +
    geom_point()


result_field %>%
    ggplot(aes(x = Body_Length , y = predictions_field , color = Sex)) +
    geom_smooth() +
    geom_point()
```

Nice to see that they are normally distributed. 


Fitting distributions??

Ratios / Percentages are not normally distributed. Weibull is a good distributions. 

Alice used weibull for the qpcr data. (paper) 

```{r}
library(fitdistrplus)
library(logspline)

result_field <- result_field %>%
dplyr::mutate(WL = predictions_field)

x <- result_field$WL

descdist(data = x, discrete = FALSE)
descdist(data = x, discrete = FALSE, #data is continuous
         boot = 1000)


```

Test for binomial distribution

```{r}
set.seed(10)
n = 25
size = 27
prob = .4
data = rbinom(x, size = size, prob = prob)
fit = fitdist(data = data, dist="binom", 
                   fix.arg=list(size = size), 
                   start=list(prob = 0.1))

summary(fit)


plot(fit)
```


```{r}
normal_ <- fitdist(x, "norm")
weibull_ <- fitdist(x, "weibull")
gamma_ <- fitdist(x, "gamma")
```
```{r}
library(fitdistrplus) # evaluate distribution

# Define function to be used to test, get the log lik and aic
tryDistrib <- function(x, distrib){
  # deals with fitdistr error:
  fit <- tryCatch(MASS::fitdistr(x, distrib), error=function(err) "fit failed")
  return(list(fit = fit,
              loglik = tryCatch(fit$loglik, error=function(err) "no loglik computed"), 
              AIC = tryCatch(fit$aic, error=function(err) "no aic computed")))
}



findGoodDist <- function(x, distribs, distribs2){
  l =lapply(distribs, function(i) tryDistrib(x, i))
  names(l) <- distribs
  print(l)
  listDistr <- lapply(distribs2, function(i){
    if (i %in% "t"){
      fitdistrplus::fitdist(x, i, start = list(df =2))
    } else {
      fitdistrplus::fitdist(x,i)
    }}
  ) 
  par(mfrow=c(2,2))
  denscomp(listDistr, legendtext=distribs2)
  cdfcomp(listDistr, legendtext=distribs2)
  qqcomp(listDistr, legendtext=distribs2)
  ppcomp(listDistr, legendtext=distribs2)
  par(mfrow=c(1,1))
}
```

```{r}
tryDistrib(x, "normal")

```

```{r}
tryDistrib(x, "binomial")
```

```{r}
tryDistrib(x, "student")
```


```{r}
tryDistrib(x, "weibull")
```

```{r , warnings = FALSE, message = FALSE}
tryDistrib(x, "weibullshifted")
```
```{r}
findGoodDist(x, "normal", "weibull")
```

```{r normal}
plot(normal_)
summary(normal_)
```
```{r gamma_}
plot(gamma_)
summary(gamma_)
```

```{r weibull_}
plot(weibull_)
summary(weibull_)
```

 ### Is alpha significant for each hypothesis?

H0: the expected load for the subspecies and between 2 groups is the same

H1: the mean load across 2 groups is the same, but can differ across subspecies

H2: the mean load across subspecies is the same, but can differ between the 2 groups

H3: the mean load can differ both across subspecies and between 2 groups


```{r}

result_field$Sex <- as.factor(result_field$Sex)

result_field <- result_field %>%
    drop_na(HI)

parasiteLoad::getParamBounds("weibull", data = result_field, response = "WL")


speparam <- c(L1start = 10,
                     L1LB = 1e-9,
                     L1UB = 20,
                     L2start = 10,
                     L2LB = 1e-9,
                     L2UB = 20,
                     alphaStart = 0, alphaLB = -5, alphaUB = 5,
                     myshapeStart = 1, myshapeLB = 1e-9, myshapeUB = 5)

##All
fitWL_Sex <- parasiteLoad::analyse(data = result_field,
                        response = "WL",
                        model = "weibull",
                        group = "Sex")

fitWL_Sex

plot_WL_Sex<- bananaPlot(mod = fitWL_Sex$H3,
             data = result_field,
             response = "WL",
             group = "Sex") +
    scale_fill_manual(values = c("blueviolet", "chartreuse2")) +
  scale_color_manual(values = c("blueviolet", "chartreuse2")) +
  theme_bw() + 
    xlab(label = "Hybrid Index") +
    ylab(label = "Weight loss (Health impact)")
 
    
    
plot_WL_Sex

HIgradientBar <- ggplot(data.frame(hi = seq(0,1,0.0001)),
                        aes(x=hi, y=1, fill = hi)) +
  geom_tile() +
  theme_void() +
  scale_fill_gradient(low = "blue", high = "red")  + 
  scale_x_continuous(expand=c(.01,0)) + 
  scale_y_continuous(expand=c(0,0)) +
  theme(legend.position = 'none')

plot_grid(plot_WL_Sex + theme(legend.position = "none") , HIgradientBar,  
                            nrow = 2, 
                            rel_heights = c(1.5, 1/8),
          align = "v")
     
```


## Summary stats for the field 

Can we test the hybrid index, WL and the infection ? 

```{r}
result_field %>%
    dplyr::group_by(MC.Eimeria) %>%
    summarize(length(Mouse_ID))
```


```{r}
result_field %>%
    dplyr::group_by(eimeriaSpecies) %>%
    summarize(length(Mouse_ID))
```

## Reproducing for melting curve


```{r}

result_field_mc <- result_field %>% 
    drop_na("MC.Eimeria")

parasiteLoad::getParamBounds("weibull", data = result_field_mc, response = "WL")


speparam <- c(L1start = 10,
                     L1LB = 1e-9,
                     L1UB = 20,
                     L2start = 10,
                     L2LB = 1e-9,
                     L2UB = 20,
                     alphaStart = 0, alphaLB = -5, alphaUB = 5,
                     myshapeStart = 1, myshapeLB = 1e-9, myshapeUB = 5)


result_field_mc <- result_field_mc %>%
    dplyr::mutate(Eimeria = case_when(
        MC.Eimeria == "TRUE" ~ "positive",
        MC.Eimeria == "FALSE" ~ "negative",
        TRUE ~ ""
    ))

result_field_mc$Eimeria <- as.factor(result_field_mc$Eimeria)


##All
fitWL_Eimeria <- parasiteLoad::analyse(data = result_field_mc,
                        response = "WL",
                        model = "weibull",
                        group = "Eimeria")
fitWL_Eimeria

plot_WL_Eimeria <- bananaPlot(mod = fitWL_Eimeria$H0,
             data = result_field_mc,
             response = "WL",
             group = "Eimeria") +
  scale_fill_manual(values = c("blue", "red")) +
  scale_color_manual(values = c("blue", "red")) +
  theme_bw()

plot_WL_Eimeria

```

## Applying the classification model on the field data



Let's see how well our predictions work!


```{r }
#The predict() function in R is used to predict the values based on the input data.
predictions_melting_field <- predict(model_melting, result_field_mc)
# assign test.data to a new object, so that we can make changes


result_melting_field <- result_field_mc
#add the new variable of predictions to the result object
result_melting_field <- cbind(result_melting_field, predictions_melting_field)


#turn the variable of melting curve into a factor so that you can compare to the predictions
result_melting_field$MC.Eimeria <- as.factor(as.character(result_melting_field$MC.Eimeria))

```





```{r }

conf_matrix_melting_field <- confusionMatrix(result_melting_field$predictions_melting_field, reference = result_melting_field$MC.Eimeria)

print(conf_matrix_melting_field)
conf_matrix_melting_field$table

plt <- as.data.frame(conf_matrix_melting_field$table)

plt$Prediction <- factor(plt$Prediction, levels=rev(levels(plt$Prediction)))
ggplot(plt, aes(x = Prediction, y =  Reference, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="forestgreen") +
        labs(x = "Predictions",y = "Reference") 
```

## Test the other model for parasite species
## Making predictions


```{r }
result_field_par <- result_field %>%
    drop_na(eimeriaSpecies) %>%
    mutate(Parasite_challenge = case_when(
        eimeriaSpecies == "Negative" ~ "uninfected",
        eimeriaSpecies == "E_ferrisi" ~ "E_ferrisi",
        eimeriaSpecies == "E_falciformis" ~ "E_falciformis",
        TRUE ~ ""
    ))  %>% 
    drop_na(Parasite_challenge)

#The predict() function in R is used to predict the values based on the input data.
predictions_parasite_field <- predict(model_Parasite, result_field_par)


#add the new variable of predictions to the result object
result_field_par <- cbind(result_field_par, predictions_parasite_field)


```




## Visualizations



```{r }
result_field_par$Parasite_challenge <- as.factor(result_field_par$Parasite_challenge)

conf_matrix_parasite <- confusionMatrix(result_field_par$predictions_parasite_field, reference = result_field_par$Parasite_challenge)



print(conf_matrix_parasite)

conf_matrix_parasite$table

plt <- as.data.frame(conf_matrix_parasite$table)
=======
plt <- as.data.frame(conf_matrix_mc$table)
>>>>>>> 9bce7f3bffa8a033f8b31bab042a0322618de2fc
plt$Prediction <- factor(plt$Prediction, levels=rev(levels(plt$Prediction)))

ggplot(plt, aes(x = Prediction, y =  Reference, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="darkturquoise") +
        labs(x = "Predictions",y = "Reference") 

```
<<<<<<< HEAD
## Next steps

Question the removal of CXCL3, maybe it exists in the wild and we need to rename
How to deal with the delta ct?

#what about instead of classifying testing with random forest for infection intensity?




##########################################################


### Let's test if we can predict infection intensity according to gene expression
## Splitting data into training and testing sets 
Splitting between training and testing:
- Assess model performance on unseen data
- Avoid over-fitting 



```{r }
#select the relevant columns:
g.imputed <- g.imputed_full %>%
    dplyr::select(c(delta, all_of(Genes))) %>%
    drop_na(delta)

g.imputed <- g.imputed %>%
    rename(CXCR3 = CXCR3_bio)

# split data into training and test

set.seed(123) # this will help us reproduce this random assignment

# in this way we can pick the random numbers

training.samples <- g.imputed$delta%>%
  createDataPartition(p = .7, # this is the partiicition! In this case 0.7 = training data and 0.3 = testing
                      list = FALSE) # we don't want to get a list in return

train.data <- g.imputed[training.samples, ] #include all the randomly selected rows
test.data <- g.imputed[-training.samples, ] 


```


=======
>>>>>>> 9bce7f3bffa8a033f8b31bab042a0322618de2fc



<<<<<<< HEAD
result_field <- result_field %>%
    drop_na(delta_ct_cewe_MminusE)

# let's see wht the ratio between acutal data and predictions
result_field <- result_field %>% 
    mutate(ratio =  delta_ct_cewe_MminusE / predictions_field)

summary(result_field$ratio)

result_field %>% 
    ggplot(aes(x = ratio)) +
    geom_histogram()

result_field2 <- result_field %>% 
    dplyr::filter(!ratio >= 6) 

result_field2 %>% 
    ggplot(aes(x = ratio)) +
    geom_histogram()


summary(result_field2$ratio)
```



## Visualizations



```{r }


result_field   %>%
  ggplot(aes(x = predictions_field, y = delta_ct_cewe_MminusE)) +
  geom_point() +
  geom_abline() +
  labs(x = "Predictions: Infection intensity", y = "Observed: Infection intensity") +
    theme_bw()
  
```


```{r}
cor(result_field$predictions_field, result_field$delta_ct_cewe_MminusE)
```
# let's try using multiplying the predictions by the ratio of 2.5

```{r}
# adding the "normalization" 
result_field2 <- result_field2 %>%
    mutate(predictions_norm = predictions_field * 2.493)

result_field2 <- result_field2  %>%
    dplyr::select(c(delta_ct_cewe_MminusE, predictions_field, predictions_norm, ratio))

cor(result_field2$predictions_norm, result_field2$delta_ct_cewe_MminusE)

cor.test(result_field2$predictions_norm, result_field2$delta_ct_cewe_MminusE,
                    method = "pearson")
result_field2   %>%
  ggplot(aes(x = predictions_norm, y = delta_ct_cewe_MminusE)) +
  geom_point() +
  labs(x = "Predictions: Infection intensity", y = "Observed: Infection intensity") +
    theme_bw()
```



=======
>>>>>>> 9bce7f3bffa8a033f8b31bab042a0322618de2fc
