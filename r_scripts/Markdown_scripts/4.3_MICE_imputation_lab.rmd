---
title: "5. Imputation of missing values"
author: "Fay"
date: '2022-10-05'
output:
  pdf_document:
    keep_md: yes
    fig_width: 12
    fig_height: 8
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load libraries

```{r}
library(mice)
library(tidyr)
library(tidyverse)
library(VIM)
library(fitdistrplus)
library(fitur)
library(visdat)
```

# Import data

```{r}
hm <- read.csv("output_data/MICE.csv")
```

```{r}
# Vectors for selecting genes

#Lab genes
# The measurements of IL.12 and IRG6 are done with an other assay and will 
#ignore for now
Gene_lab   <- c("IFNy", "CXCR3", "IL.6", "IL.13", "IL.10",
                "IL1RN","CASP1", "CXCL9", "IDO1", "IRGM1", "MPO", 
                "MUC2", "MUC5AC", "MYD88", "NCR1", "PRF1", "RETNLB", "SOCS1", 
                "TICAM1", "TNF") #"IL.12", "IRG6")

Genes_wild   <- c("IFNy", "CXCR3", "IL.6", "IL.13", "IL.10", 
                  "IL1RN","CASP1", "CXCL9", "IDO1", "IRGM1", "MPO", 
                  "MUC2", "MUC5AC", "MYD88", "NCR1", "PRF1", "RETNLB", "SOCS1", 
                  "TICAM1", "TNF") #, "IL.12", "IRG6")

Facs_lab <- c("CD4", "Treg", "Div_Treg", "Treg17", "Th1", 
                    "Div_Th1", "Th17", "Div_Th17", "CD8", "Act_CD8", 
                    "Div_Act_CD8", "IFNy_CD4", "IFNy_CD8")

#"IL17A_CD4")  
#,"Treg_prop", from another assay

Facs_wild <- c( "Treg", "CD4", "Treg17", "Th1", "Th17", "CD8",
                     "Act_CD8", "IFNy_CD4", "IL17A_CD4", "IFNy_CD8")

```


# How do the variables look like? 

1. cleaning 
```{r}

#how many nas in each column
#sapply(hm, function(x) sum(is.na(x)))

# Required step for the further imputations 
hm  <- hm %>% mutate_if(is.character, as.factor)
hm <- hm %>% mutate_if(is.integer, as.numeric)

```


## Test different distributions 

After talking with Dan and some reading, I found out that many machine learning 
models / packages require data following a normal distribution. I am hear trying 
to standardize my data to a normal distribution. After testing the MICE package 
on my standardized and on the raw data, I realized there was no difference in 
my end results. 

I would like to further question if normalization / scaling of the data is necessary. 

I am here using the laboratory infections, as we have more measures and I am 
randomly selecting a variable to represent the facs measures and another gene 
for the gene expression data. In some papers it is mentioned that facs / genes 
may each follow different distribution (adding to my confusion).

```{r}

# I am here selecting CD4 to represent the facs datta
facs_variable <- hm %>% 
  filter(origin == "Lab", dpi == max_dpi, infection == "challenge") %>% 
  dplyr::select(CD4) 

# we don't want any nas as these will disrupt the distribution tests by different 
# packages
facs_variable <- facs_variable %>% drop_na()

#here is our facs variabls
a <- facs_variable$CD4

# gene_variable, another random gene (MYD88), chosen due to abundancy in measures
gene_variable <- hm %>%
  filter(origin == "Lab", dpi == max_dpi, infection == "challenge") %>% 
  dplyr::select(MYD88)

# Let's drop the nas again
gene_variable <- gene_variable %>% 
  drop_na()

#s and hurray here is our gene variable
b <- gene_variable$MYD88
```

# Fucntions to test distributions

I think I got these from Alice's package ParasiteLoad (Have to check again)

```{r}

# Define function to be used to test, get the log lik and aic
tryDistrib <- function(x, distrib){
  # deals with fitdistr error:
  fit <- tryCatch(MASS::fitdistr(x, distrib), error=function(err) "fit failed")
  return(list(fit = fit,
              loglik = tryCatch(fit$loglik, error=function(err) "no loglik computed"), 
              AIC = tryCatch(fit$aic, error=function(err) "no aic computed")))
}



findGoodDist <- function(x, distribs, distribs2){
  l =lapply(distribs, function(i) tryDistrib(x, i))
  names(l) <- distribs
  print(l)
  listDistr <- lapply(distribs2, function(i){
    if (i %in% "t"){
      fitdistrplus::fitdist(x, i, start = list(df =2))
    } else {
      fitdistrplus::fitdist(x,i)
    }}
  ) 
  par(mfrow=c(2,2))
  denscomp(listDistr, legendtext=distribs2)
  cdfcomp(listDistr, legendtext=distribs2)
  qqcomp(listDistr, legendtext=distribs2)
  ppcomp(listDistr, legendtext=distribs2)
  par(mfrow=c(1,1))
}

```


# For the facs data

Testing our CD4s distributions

```{r}
set.seed(333)
descdist(data = a, discrete = FALSE, boot = 1000)

```

So seeing this graph, I understand we could be having a beta distribution.. 
But what happens next, shows something different. 

Interface for looking at distributions

```{r warning=FALSE, message=FALSE}

#fitur::fit_dist_addin()
```

According to interface, I can visually identify a cauchy distribution.



```{r}
tryDistrib(a, "normal") #yes
tryDistrib(a, "binomial") #nope
tryDistrib(a, "student") #nope
tryDistrib(a, "weibull") #yes
tryDistrib(a, "weibullshifted") #nope
tryDistrib(a, "gamma") #nope
tryDistrib(a, "cauchy") #yes
tryDistrib(a, "exp") #nope
tryDistrib(a, "log") #nope
tryDistrib(a, "t")

```

FACS data --> cauchy?


################

Gene data

```{r}

ggplot(gene_variable, aes(MYD88)) +
  geom_histogram()


set.seed(66)
descdist(data = b, discrete = FALSE, boot = 1000)

```


Interface for looking at the distributions. #really cool

```{r warning=FALSE, message=FALSE}

#fitur::fit_dist_addin()
```


# For the gene data


```{r}
tryDistrib(b, "normal") #yes
tryDistrib(b, "binomial") #nope
tryDistrib(b, "student") #nope
tryDistrib(b, "weibull") #yes
tryDistrib(b, "weibullshifted") #nope
tryDistrib(b, "gamma") #yes
tryDistrib(b, "cauchy") #yes
tryDistrib(b, "exp") #nope
tryDistrib(b, "log") #nope
tryDistrib(b, "t") #nope
```
```{r}
findGoodDist(b, "normal",  "weibull")
findGoodDist(b, "normal",  "cauchy")
```
Cauchy? 


For some reason I am seeing Cauchy distributions everywhere. Please help me. 



# Standardization

I am now ignoring this part as the standardization didn't have much of an impact 
in my imputations.

Transforming the features to have the properties of a standard normal distribu-
tion with mean = 0 and standard deviation = 1

I have tried the imputations with the standardized and non-standardized data 
and I am getting the same results. Therefore I am ditching this "standardization".

```{r}

# function to standardize data
#standardize <- function(x) {
#  return ((x - mean(x, na.rm = TRUE)) /
#            sd(x, na.rm = TRUE))
#}


#summary(x) # facs
#summary(y) # gene

#testing the function
#x_stand <- standardize(x)

#fitur::fit_dist_addin()

#summary(x_stand)

# create a data frame with only the variables to standardize
#genes_facs_df <- hm %>% 
#  dplyr::select(all_of(c(Facs_lab, Gene_lab, Facs_wild, Genes_wild)))

# apply the standardize function on all the numeric variables 
#std_data <- as.data.frame(lapply(genes_facs_df, standardize))

#colnames(std_data) <- paste(colnames(std_data), "std", sep = "_")

# join the standardized data to our data set
#hm <- cbind(hm, std_data)

#remove the non-standardized data

#hm <- hm %>%
 # dplyr::select(-all_of(c(Facs_lab, Gene_lab, Facs_wild, Genes_wild)))

```



## Imputing missing data 


Here is a beautiful graphic guide for my imputations. 
https://stefvanbuuren.name/fimd/sec-toomany.html



I will be using the package MICE (multivariate Imputation by chained 
Equations) which only requires a data frame of missing observations.

Description: *Multiple imputation using Fully Conditional Specification (FCS)*

implemented by the MICE algorithm as described in Van Buuren and
Groothuis-Oudshoorn (2011) <doi:10.18637/jss.v045.i03>. Each variable has
its own imputation model. Built-in imputation models are provided for
continuous data (predictive mean matching, normal), binary data (logistic
regression), unordered categorical data (polytomous logistic regression)
and ordered categorical data (proportional odds). MICE can also impute
continuous two-level data (normal model, pan, second-level variables).
Passive imputation can be used to maintain consistency between variables.
Various diagnostic plots are available to inspect the quality of the
imputations.

https://www.jstatsoft.org/article/view/v045i03

tutorial: https://www.youtube.com/watch?v=WPiYOS3qK70

https://datascienceplus.com/imputing-missing-data-with-r-mice-package/


https://datascienceplus.com/handling-missing-data-with-mice-package-a-simple-approach/



### Missing data can be classified into three categories:

#### 1. Missing completely at random (MCAR)
  
We can't probably predict that value from any other value in the data. 
MCAR implies the reason for the missingness of a field is completely random, 
and that we probably can't predict that value from any other value in the data. 
  
#### 2. Missing at Random (MAR)

Missingess can be explained by other values in other columns, but not from
that column.
  
#### 3. Missing NOT at random (MNAR)


The basic MICE assumption is that the data is missing at random, and that we can 
make a guess about its true value by looking at other data samples.


## Step1 : cleaning and checking the missing data points in our field data. 


```{r impute_missing}

hm %>%
  aggr(col = c('navyblue', 'red'), numbers = TRUE, sortVars = TRUE, 
       labels=names(hm), cex.axis=.7, gap=3, 
       ylab=c("Histogram of missing data","Pattern"))
       
       
marginplot(hm[c(35,38)])

```


## Now let's coninue by ussing the package MICE to impute the data
# Lab

I first used standardized data. Now, i added # to remove these steps.

*Now using raw data!*

Stef Van Buuren advises of using up to 15 - 20 predictor variables. I have tried 
using the whole data set, which created chaos. 

I am now using everything possible to use, even the variables that I will use in 
my further analysis. 

- Further reading required here.


I devided the data into lab and field again, as it is only adding to the con


## Lab Genes
```{r }
# genes
# lab samples
lab <- hm %>%
  filter(origin == "Lab", infection == "challenge", dpi == dpi_max)

lab <- unique(lab)

gf_lab <- lab %>% 
  dplyr::select(all_of(Gene_lab))

gf_lab <- unique(gf_lab)

#remove rows with only nas
gf_lab <- gf_lab[,colSums(is.na(gf_lab))<nrow(gf_lab)]

#remove colums with only nas 
gf_lab <- gf_lab[rowSums(is.na(gf_lab)) != ncol(gf_lab), ]

#remove wrongly normalized genes
lab <- lab %>%
  dplyr::select(-ends_with("_N"))


#select same rows in the first table
lab_gene <- lab[row.names(gf_lab), ]


lab_gene[rowSums(is.na(lab_gene)) != ncol(lab_gene), ]

# really removing empty columns
lab_gene <- lab_gene %>% 
  discard(~all(is.na(.) | . ==""))


# looking at patterns of nas
pattern_na <-as.data.frame(md.pattern(lab_gene))


# genes
#select the relevant columns to use for the imputation
lab_genes <- lab_gene %>%
  dplyr::select(c(Mouse_ID, experiment, primary_infection, challenge_infection, 
                  mouse_strain, weight, weight_dpi0, relative_weight, 
                  oocyst_sq1, oocyst_sq2, oocyst_sq3, oocyst_sq4, OO4sq, OOC, 
                  MC.Eimeria, delta_ct_cewe_MminusE, IFNy_CEWE, IFNy_MES, 
                  all_of(Gene_lab)))


# The frequency distribution of the missing cases per variable can be obtained 
# as:
init <- mice(lab_genes, maxit = 0)


#we want to impute only the specific variables
meth <- init$method

vis_miss(lab_genes)

#select all the colnames ending in std (the standardized ones)
#std <- colnames(lab %>% dplyr::select(ends_with("_std"))) 

# set every variable that is not one of your variables of interest to ""
#You can supply a vector to the method argument of mice::mice. This vector should contain the methods that you want to use to impute the variables you want to impute. In the example they first do a dry-run 
#meth[!(names(meth) %in% all_of(std))] <- ""

# repeat the imputation only for the specific variables
#init <- mice(lab, maxit = 0, method = meth)

# table of amount of variables with the amount of missing values 
#table(init$nmis)

# which method is used for imputation? In this case the package mice 
# uses the default method for continuous variable, 
# which is pmm, or predictive mean matching

# now impute the data and save it as the oject: 
# igf

#vis_miss(lab)

#sapply(lab, function(x) sum(is.na(x)))

# which column numbers end in Std 
#grep("_std", colnames(lab) )

#imp <- mice(lab, print = FALSE)


# m=5 refers to the number of imputed datasets. Five is the default value.
igf <- mice(lab_genes, m = 5, seed = 500) # method = meth,
summary(igf)

# to check each column with imputed data
## igf$imp$IFNy

#Now we can get back the completed dataset using the complete()
complete_lab_gene <- complete(igf, 1)

#visualize missingness
vis_miss(complete_lab_gene)

#sapply(complete_lab, function(x) sum(is.na(x)))

imp_lab_gene <- complete_lab_gene %>%
  dplyr::select(all_of(Gene_lab))

#add an ending to the imputed columns
colnames(imp_lab_gene) <- paste(colnames(imp_lab_gene), "imp", sep = "_")

lab_gene <- lab_gene %>% 
  dplyr::select(Mouse_ID)

#now join it to the full data set of the laboratory infections
lab_gene <- cbind(lab_gene, imp_lab_gene)

lab_gene <- unique(lab_gene) 

lab <- lab %>%
  left_join(lab_gene, by = "Mouse_ID")


lab <- unique(lab)

```


Now repeat for the lab facs


## Lab Facs
```{r }
gf_lab <- lab %>% 
  dplyr::select(all_of(Facs_lab))

#remove rows with only nas
gf_lab <- gf_lab[,colSums(is.na(gf_lab))<nrow(gf_lab)]

#remove colums with only nas 
gf_lab <- gf_lab[rowSums(is.na(gf_lab)) != ncol(gf_lab), ]


vis_dat(gf_lab)

# no need to impute



write.csv(lab, "output_data/Lab_imputed.csv", row.names = FALSE)
```



